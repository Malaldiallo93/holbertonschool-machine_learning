# Policy Gradients

# In this project, you will implement your own Policy Gradient in your loop of reinforcement learning (by using the Monte-Carlo policy gradient algorithm - also called REINFORCE).

Resources
Read or watch:

How Policy Gradient Reinforcement Learning Works
Policy Gradients in a Nutshell
RL Course by David Silver - Lecture 7: Policy Gradient Methods
Reinforcement Learning 6: Policy Gradients and Actor Critics
Policy Gradient Algorithms
Learning Objectives
What is Policy?
How to calculate a Policy Gradient?
What and how to use a Monte-Carlo policy gradient?
Requirements
General
Allowed editors: vi, vim, emacs
All your files will be interpreted/compiled on Ubuntu 20.04 LTS using python3 (version 3.9)
Your files will be executed with numpy (version 1.25.2), and gymnasium (version 0.29.1)
All your files should end with a new line
The first line of all your files should be exactly #!/usr/bin/env python3
A README.md file, at the root of the folder of the project, is mandatory
Your code should use the pycodestyle style (version 2.11.1)
All your modules should have documentation (python3 -c 'print(__import__("my_module").__doc__)')
All your classes should have documentation (python3 -c 'print(__import__("my_module").MyClass.__doc__)')
All your functions (inside and outside a class) should have documentation (python3 -c 'print(__import__("my_module").my_function.__doc__)' and python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)')
All your files must be executable
Your code should use the minimum number of operations
