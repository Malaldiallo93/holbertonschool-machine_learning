# Attention mechanism

Attention mechanism is a mechanism that allows a model to focus on the most relevant parts of the input when making predictions. It is a concept that was inspired by the human brain, which is able to focus on a specific part of an image or a sentence when processing information.

In the context of deep learning, attention mechanism is used in various tasks such as machine translation, image captioning, and speech recognition. It allows the model to learn to focus on the most relevant parts of the input data, which can improve the performance of the model.

In this section, we will explore the attention mechanism in more detail, including how it works, its applications, and how to implement it in deep learning models.

Resources:
Read or watch:

Attention Model Intuition
Attention Model
How Transformers work in deep learning and NLP: an intuitive introduction
Transformers
Bert, GPT : The Illustrated GPT-2 - Visualizing Transformer Language Models
SQuAD
Glue
Self supervised learning
Learning Objectives
At the end of this project, you are expected to be able to explain to anyone, without the help of Google:

General
What is the attention mechanism?
How to apply attention to RNNs
What is a transformer?
How to create an encoder-decoder transformer model
What is GPT?
What is BERT?
What is self-supervised learning?
How to use BERT for specific NLP tasks
What is SQuAD? GLUE?