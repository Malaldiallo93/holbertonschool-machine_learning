Resources
Read or watch:

Hyperparameter (machine learning)
Feature scaling
Why, How and When to Scale your Features
Normalizing your data
Moving average
An overview of gradient descent optimization algorithms
A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size
Stochastic Gradient Descent with momentum
Understanding RMSprop
Adam
Learning Rate Schedules
deeplearning.ai videos (Note: I suggest watching these video at 1.5x - 2x speed):
Normalizing Inputs
Mini Batch Gradient Descent
Understanding Mini-Batch Gradient Descent
Exponentially Weighted Averages
Understanding Exponentially Weighted Averages
Bias Correction of Exponentially Weighted Averages
Gradient Descent With Momentum
RMSProp
Adam Optimization Algorithm
Learning Rate Decay
Normalizing Activations in a Network
Fitting Batch Norm Into Neural Networks
Why Does Batch Norm Work?
Batch Norm At Test Time
The Problem of Local Optima
References:

numpy.random.permutation
tf.nn.moments
tf.train.MomentumOptimizer
tf.train.RMSPropOptimizer
tf.train.AdamOptimizer
tf.nn.batch_normalization
tf.train.inverse_time_decay
Learning Objectives
At the end of this project, you are expected to be able to explain to anyone, without the help of Google:

General
What is a hyperparameter?
How and why do you normalize your input data?
What is a saddle point?
What is stochastic gradient descent?
What is mini-batch gradient descent?
What is a moving average? How do you implement it?
What is gradient descent with momentum? How do you implement it?
What is RMSProp? How do you implement it?
What is Adam optimization? How do you implement it?
What is learning rate decay? How do you implement it?
What is batch normalization? How do you implement it?