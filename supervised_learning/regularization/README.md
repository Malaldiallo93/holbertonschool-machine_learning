Regularization

Resources
Read or watch:

Regularization (mathematics)
An Overview of Regularization Techniques in Deep Learning (up to A case study on MNIST data with keras excluded)
L2 Regularization and Back-Propagation
Intuitions on L1 and L2 Regularisation
Analysis of Dropout
Early stopping
How to use early stopping properly for training deep neural network?
Data Augmentation | How to use Deep Learning when you have Limited Dataâ€Š
deeplearning.ai videos (Note: I suggest watching these video at 1.5x - 2x speed):
Regularization
Why Regularization Reduces Overfitting
Dropout Regularization
Understanding Dropout
Other Regularization Methods
References:

numpy.linalg.norm
numpy.random.binomial
tf.keras.regularizers.L2
tf.layers.Dense
tf.losses.get_regularization_loss
tf.layers.Dropout
Dropout: A Simple Way to Prevent Neural Networks from Overfitting
Early Stopping - but when?
L2 Regularization versus Batch and Weight Normalization
Learning Objectives
At the end of this project, you are expected to be able to explain to anyone, without the help of Google:

General
What is regularization? What is its purpose?
What is are L1 and L2 regularization? What is the difference between the two methods?
What is dropout?
What is early stopping?
What is data augmentation?
How do you implement the above regularization methods in Numpy? Tensorflow?
What are the pros and cons of the above regularization methods?
