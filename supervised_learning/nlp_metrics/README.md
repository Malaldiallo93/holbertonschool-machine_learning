# NLP metrics

## BLEU score

BLEU (Bilingual Evaluation Understudy) is a metric for evaluating a generated sentence to a reference sentence. It is based on the precision of n-grams in the generated sentence compared to the reference sentence. BLEU score is a number between 0 and 1, where 1 means the generated sentence is identical to the reference sentence.

Resources
Read or watch:

7 Applications of Deep Learning for Natural Language Processing
10 Applications of Artificial Neural Networks in Natural Language Processing
A Gentle Introduction to Calculating the BLEU Score for Text in Python
Bleu Score
Evaluating Text Output in NLP: BLEU at your own risk
ROUGE metric
Evaluation and Perplexity
Evaluation metrics
Definitions to skim

BLEU
ROUGE
Perplexity
References:

BLEU: a Method for Automatic Evaluation of Machine Translation (2002)
ROUGE: A Package for Automatic Evaluation of Summaries (2004)
